{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV3CMsPMn1s2"
   },
   "source": [
    "As per notebook convention, imports are handled at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available:  True\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from statistics import mean\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "print(\"Cuda is available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data\n",
    "## Preprocessing\n",
    "This the data preprocessing \"pipeline\". These single responsibility unary functions are used to transform the\n",
    "raw data collected and split via the scala spark job into usable source and target tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# reads spark files and puts them into lines\n",
    "def read_multi_json_objects_file(filename: str) -> List[dict]:\n",
    "    with open(filename, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "# splits lines into user -> (history+target)\n",
    "def extract_raw_history_data_from(data):\n",
    "    return [history[\"value\"] for history in data ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def l2_normalization(data: Tensor) -> Tensor:\n",
    "    return F.normalize(data, p=2, dim=2)\n",
    "\n",
    "# creates tensors from history\n",
    "def as_tensor(bert_histories: List[List[List[float]]]) -> Tensor:\n",
    "\n",
    "    # essentially create one giant batch\n",
    "    histories = [ torch.tensor(history).unsqueeze(1) for history in bert_histories ]\n",
    "    return torch.cat(histories, 1) # either normalize or not, subject to testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pert_padding_vector = torch.zeros(768).to(device)\n",
    "\n",
    "# this has to be precalculated as per-batch calculation doubles traning time\n",
    "# creates a nxm matrix\n",
    "def generate_padding_mask(data: Tensor) -> Tensor:\n",
    "            batch_paddings = []\n",
    "\n",
    "            for batch in range(data.shape[1]):\n",
    "                sequence_padding = torch.tensor([\n",
    "                    torch.eq(data[article, batch, :], pert_padding_vector).all() # true if padding\n",
    "                        for article in range(data.shape[0])\n",
    "                ]).to(device)\n",
    "                batch_paddings.append(sequence_padding)\n",
    "\n",
    "            return torch.stack(batch_paddings, dim=0).detach().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our data is prepared using the functions implemented above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# shape of these is [x0, ..., xn + 1], so 2 longer than seq_length\n",
    "# this is because we're going to split at 0-15, 1-16, 2-17 for encoder S, decoder Sd and target St respectively\n",
    "train_test_data = read_multi_json_objects_file('../data/seq_16/train/6k.json')\n",
    "validation_data = read_multi_json_objects_file('../data/seq_16/val/600.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_test_histories = extract_raw_history_data_from(train_test_data)\n",
    "validation_histories = extract_raw_history_data_from(validation_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train and test splits are defined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "h5nNBvr05Z7X",
    "outputId": "c00d04e4-070e-4899-f0d7-be41e7a593a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_split = 0.8\n",
    "\n",
    "samples = len(train_test_histories)\n",
    "\n",
    "train_samples = int(samples * train_test_split)\n",
    "val_samples = samples - train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "unbatched_train_data = train_test_histories[:train_samples]\n",
    "unbatched_test_data = train_test_histories[train_samples:]\n",
    "unbatched_val_data = validation_histories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Expand and push data to cuda device\n",
    "train_data  = as_tensor(unbatched_train_data).to(device)\n",
    "test_data   = as_tensor(unbatched_test_data).to(device)\n",
    "val_data    = as_tensor(unbatched_val_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Precompute padding maps and push to cuda device\n",
    "train_padding_map = generate_padding_mask(train_data).to(device)\n",
    "test_padding_map  = generate_padding_mask(test_data).to(device)\n",
    "val_padding_map   = generate_padding_mask(val_data).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## Positional Encoding\n",
    "RelativePositionalEncoding module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use sine and cosine functions of different frequencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class RelativePositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, model_dimension, dropout_rate, max_len=5000):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, model_dimension)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dimension, 2) * -(math.log(10000.0) / model_dimension))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, unencoded):\n",
    "        encoded = unencoded + Variable(self.pe[:, :unencoded.size(1)], requires_grad=False)\n",
    "        return self.dropout(encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder/Decoder definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None, src_key_padding_mask: Tensor = None) -> Tuple[Tensor, Tensor]:\n",
    "        res = src\n",
    "        src = self.norm1(src)\n",
    "        src, attn = self.self_attn.forward(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        src = res + self.dropout1(src)\n",
    "\n",
    "        res = src\n",
    "        src = self.norm2(src)\n",
    "        src = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = res + self.dropout2(src)\n",
    "\n",
    "        return src, attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor = None, src_key_padding_mask: Tensor = None) -> Tuple[Tensor, Tensor]:\n",
    "        output = src\n",
    "        n_self_attn = []\n",
    "        for mod in self.layers:\n",
    "            output, self_attn = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "            n_self_attn.append(self_attn.unsqueeze(dim=0))\n",
    "\n",
    "        return output, torch.cat(n_self_attn, 0).detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor = None, memory_mask: Tensor = None,\n",
    "                tgt_key_padding_mask: Tensor = None, memory_key_padding_mask: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        res = tgt\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt, self_attn = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = res + self.dropout1(tgt)\n",
    "\n",
    "        res = tgt\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt, mult_attn = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = res + self.dropout2(tgt)\n",
    "\n",
    "        res = tgt\n",
    "        tgt = self.norm3(tgt)\n",
    "        tgt = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = res + self.dropout3(tgt)\n",
    "\n",
    "        return tgt, self_attn, mult_attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor = None,\n",
    "                memory_mask: Tensor = None, tgt_key_padding_mask: Tensor = None,\n",
    "                memory_key_padding_mask: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output, self_attn, mult_attn = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        return output, self_attn.detach(), mult_attn.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input/Output definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Input(nn.Module):\n",
    "    def __init__(self, model_dimension, embedding_dimension, dropout):\n",
    "        super().__init__()\n",
    "        self.linear              = nn.Linear(embedding_dimension, model_dimension)\n",
    "        self.positional_encoding = RelativePositionalEncoding(model_dimension, dropout)\n",
    "\n",
    "    def forward(self, data) -> Tensor:\n",
    "        data = l2_normalization(data)\n",
    "        data = self.linear(data) * math.sqrt(model_dimension)\n",
    "        data = self.positional_encoding(data)\n",
    "        return data\n",
    "\n",
    "class Output(nn.Module):\n",
    "    def __init__(self, model_dimension, feedforward_dimension, embedding_dimension, dropout):\n",
    "        super().__init__()\n",
    "        self.widen           = nn.Linear(model_dimension, feedforward_dimension)\n",
    "        self.relu            = nn.ReLU()\n",
    "        self.dropout         = nn.Dropout(dropout)\n",
    "        self.shrink          = nn.Linear(feedforward_dimension, embedding_dimension)\n",
    "        self.hard_shrink     = nn.Hardshrink(0.01)\n",
    "\n",
    "    def forward(self, data) -> Tensor:\n",
    "        data = self.widen(data)\n",
    "        data = self.relu(data)\n",
    "        data = self.dropout(data)\n",
    "        data = self.shrink(data)\n",
    "        data = self.hard_shrink(data)\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class BertHistoryTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_seq_length,\n",
    "        embedding_dimension,\n",
    "        model_dimension,\n",
    "        encoder_layers,\n",
    "        decoder_layers,\n",
    "        heads,\n",
    "        dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.padding = torch.zeros(embedding_dimension).to(device)\n",
    "        self.mask = self.generate_square_subsequent_mask(decoder_seq_length)\n",
    "\n",
    "        self.pre_ln_norm = nn.LayerNorm(model_dimension).to(device)\n",
    "\n",
    "        self.encoder = Input(model_dimension, embedding_dimension, dropout).to(device)\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=model_dimension,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=feedforward_dimension,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_enc = TransformerEncoder(encoder_layer, encoder_layers).to(device)\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=model_dimension,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=feedforward_dimension,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_dec = TransformerDecoder(decoder_layer, decoder_layers).to(device)\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        self.decoder = Output(model_dimension, feedforward_dimension, embedding_dimension, dropout).to(device)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    def transform(self, src, tgt) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        memory, enc_att           = self.transformer_enc.forward(src)\n",
    "        memory, dec_att, mult_att = self.transformer_dec.forward(tgt, self.pre_ln_norm(memory), tgt_mask=self.mask)\n",
    "\n",
    "        return memory, enc_att, dec_att, mult_att\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    def _step(self, src, tgt, src_key_padding_mask=None) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        source_memory = self.encoder(src)\n",
    "        target_memory = self.encoder(tgt)\n",
    "\n",
    "        memory, enc_att, dec_attn, mult_attn = self.transform(source_memory, target_memory)\n",
    "        output = self.decoder(memory)\n",
    "\n",
    "        return output, enc_att, dec_attn, mult_attn\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None) -> Tensor:\n",
    "        return self._step(src, tgt, src_key_padding_mask)[0]\n",
    "\n",
    "    def predict(self, src, tgt, src_key_padding_mask=None) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            return self._step(src, tgt, src_key_padding_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batching"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Semantically part of the training loop, this function stacks tensors of size _seq-length x channels_ into tensors of\n",
    "dimensions _seq-length x batch-size x channels_, adhering to the 'named tensor' convention."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# batchifies and expands source to encoder source, decoder source, target, and padding\n",
    "def get_batch(source: Tensor, src_padding: Optional[Tensor], batch_offset) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    current_batch_size = min(batch_size, source.shape[1] - 1 - batch_offset)\n",
    "\n",
    "    # source is already sliced per sequence, so we address batches only by offset\n",
    "    batch: Tensor   = source[:, batch_offset : batch_offset + current_batch_size, :]\n",
    "    if src_padding != None:\n",
    "        padding: Tensor = src_padding[batch_offset : batch_offset + current_batch_size, :]\n",
    "    else:\n",
    "        padding: Tensor = pert_padding_vector\n",
    "\n",
    "    # calculate offsets for decoder and target sequence\n",
    "    dec_start = max_seq_length - decoder_seq_length - 1\n",
    "    dec_end   = dec_start + decoder_seq_length\n",
    "\n",
    "    sources_enc: Tensor = batch[0            :seq_length , :, :]\n",
    "    sources_dec: Tensor = batch[dec_start    :dec_end    , :, :]\n",
    "    targets: Tensor     = batch[dec_start + 1:dec_end + 1, :, :]\n",
    "\n",
    "    # Careful: padding has length of scala seq length, so it has to be sliced\n",
    "    return sources_enc, sources_dec, targets, padding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# LOSS FUNCTION----------------------------------------------------------------\n",
    "cosine_loss = nn.CosineEmbeddingLoss()\n",
    "eucl_loss = nn.MSELoss()\n",
    "\n",
    "def calculate_loss(pred: Tensor, target: Tensor) -> Tensor:\n",
    "    #return cosine_loss(\n",
    "    #    pred.reshape(-1, embedding_dimension),\n",
    "    #    target.reshape(-1, embedding_dimension),\n",
    "    #    target=similar_dissimilar_label[:(pred.shape[0]) * pred.shape[1]]\n",
    "    #)\n",
    "    #return torch.sqrt(eucl_loss(pred.reshape(-1), target.reshape(-1)) + 1e-6)\n",
    "    return torch.sqrt(\n",
    "        eucl_loss( # all but the last item\n",
    "            pred[:decoder_seq_length-1, : , :].reshape(-1),\n",
    "            target[:decoder_seq_length-1, : , :].reshape(-1)\n",
    "            ) + 1e-6 # plus epsilon for RMSE\n",
    "    ) + (1.3 * torch.sqrt(\n",
    "        eucl_loss( # the last item - our real prediction - is weighted more heavily\n",
    "            pred[decoder_seq_length-1:decoder_seq_length, : , :].reshape(-1),\n",
    "            target[decoder_seq_length-1:decoder_seq_length, : , :].reshape(-1),\n",
    "        ) + 1e-6 # plus epsilon for RMSE\n",
    "    ))\n",
    "    #return cosine_loss(\n",
    "    #    pred.reshape(-1, embedding_dimension),\n",
    "    #    target.reshape(-1, embedding_dimension),\n",
    "    #    target=similar_dissimilar_label[:(pred.shape[0]) * pred.shape[1]]\n",
    "    #) + eucl_loss(pred.reshape(-1), target.reshape(-1))\n",
    "    #return mse_loss.forward(\n",
    "    #    pred[:seq_length-1, : , :].reshape(-1),\n",
    "    #    target[:seq_length-1, : , :].reshape(-1),\n",
    "    #) + cosine_loss.forward(\n",
    "    #    pred[seq_length-1:seq_length, : , :].reshape(-1, embedding_dimension),\n",
    "    #    target[seq_length-1:seq_length, : , :].reshape(-1, embedding_dimension),\n",
    "    #    target=similar_dissimilar_label[:pred.shape[1]]\n",
    "    #)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and evaluation loops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = []\n",
    "    start_time = time.time()\n",
    "    log_interval = max(1, train_data.shape[1] / batch_size // 2)\n",
    "\n",
    "\n",
    "\n",
    "    for batch_number, batch_offset in enumerate(range(0, train_data.shape[1] - 1, batch_size)):\n",
    "        enc_source, dec_source, target, pad = get_batch(train_data, train_padding_map, batch_offset)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output: Tensor = model.forward(enc_source, dec_source, None)\n",
    "\n",
    "        current_loss: Tensor = calculate_loss(output, target)\n",
    "\n",
    "        current_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(current_loss.detach().item())\n",
    "\n",
    "        if batch_number + 1 % log_interval == 0:\n",
    "            cur_loss = mean(total_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                              'lr {:02.6f} | ms/batch {:5.2f} | '\n",
    "                              'loss {:5.4f} | ppl {:8.2f}'.format(\n",
    "                                    epoch,\n",
    "                                    batch_number + 1,\n",
    "                                    train_data.shape[1] // batch_size,\n",
    "                                    scheduler.get_last_lr()[0],\n",
    "                                    elapsed * 1000 / log_interval,\n",
    "                                    cur_loss,\n",
    "                                    math.exp(cur_loss)\n",
    "                                )\n",
    "            )\n",
    "            total_loss = [0]\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model: BertHistoryTransformer, data_source: Tensor) -> Tuple[float, Tensor, Tensor, Tensor]:\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_offset in enumerate(range(0, data_source.shape[1] - 1, batch_size)):\n",
    "            enc_source, dec_source, target, pad = get_batch(data_source, None, batch_offset)\n",
    "            output, enc_attn, dec_attn, mult_attn = eval_model.predict(enc_source, dec_source, pad)\n",
    "\n",
    "            current_loss: Tensor = calculate_loss(output, target).detach().item()\n",
    "\n",
    "            total_loss.append(current_loss)\n",
    "    return mean(total_loss), enc_attn, dec_attn, mult_attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters and Hyperparameters, Model instantiation, Loss defintion\n",
    "Training is started"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ==== Got   4800 training examples, splitting into   37 batches a  128 examples \n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.59s | test loss 5.9339 | test ppl   377.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 14.07s | test loss 5.6964 | test ppl   297.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 14.10s | test loss 5.3617 | test ppl   213.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 14.11s | test loss 4.9563 | test ppl   142.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.13s | test loss 4.5157 | test ppl    91.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 14.19s | test loss 4.0794 | test ppl    59.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 14.17s | test loss 3.6842 | test ppl    39.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 14.19s | test loss 3.3534 | test ppl    28.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 14.19s | test loss 3.0880 | test ppl    21.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 14.21s | test loss 2.8738 | test ppl    17.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 14.21s | test loss 2.6943 | test ppl    14.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 14.19s | test loss 2.5406 | test ppl    12.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 14.23s | test loss 2.4122 | test ppl    11.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 14.21s | test loss 2.2975 | test ppl     9.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 14.21s | test loss 2.2043 | test ppl     9.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 14.22s | test loss 2.1225 | test ppl     8.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 14.24s | test loss 2.0547 | test ppl     7.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 14.14s | test loss 1.9982 | test ppl     7.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 14.31s | test loss 1.9520 | test ppl     7.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 14.26s | test loss 1.9189 | test ppl     6.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 14.27s | test loss 1.8931 | test ppl     6.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 14.27s | test loss 1.8759 | test ppl     6.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 14.29s | test loss 1.8670 | test ppl     6.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 14.27s | test loss 1.8666 | test ppl     6.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 14.27s | test loss 1.8705 | test ppl     6.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 14.27s | test loss 1.8779 | test ppl     6.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 14.29s | test loss 1.8816 | test ppl     6.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 14.26s | test loss 1.8866 | test ppl     6.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 14.27s | test loss 1.8810 | test ppl     6.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 14.28s | test loss 1.8689 | test ppl     6.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 14.26s | test loss 1.8502 | test ppl     6.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 14.27s | test loss 1.8299 | test ppl     6.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 14.27s | test loss 1.7994 | test ppl     6.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 14.27s | test loss 1.7643 | test ppl     5.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 14.25s | test loss 1.7295 | test ppl     5.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 14.26s | test loss 1.6888 | test ppl     5.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 14.26s | test loss 1.6528 | test ppl     5.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 14.24s | test loss 1.6087 | test ppl     5.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 14.29s | test loss 1.5706 | test ppl     4.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 14.18s | test loss 1.5286 | test ppl     4.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 14.16s | test loss 1.4839 | test ppl     4.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 14.19s | test loss 1.4463 | test ppl     4.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 14.17s | test loss 1.4170 | test ppl     4.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 14.17s | test loss 1.3776 | test ppl     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 14.17s | test loss 1.3510 | test ppl     3.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 14.17s | test loss 1.3178 | test ppl     3.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 14.21s | test loss 1.2913 | test ppl     3.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 14.20s | test loss 1.2631 | test ppl     3.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 14.20s | test loss 1.2379 | test ppl     3.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 14.18s | test loss 1.2138 | test ppl     3.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 14.18s | test loss 1.1875 | test ppl     3.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 14.17s | test loss 1.1660 | test ppl     3.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 14.18s | test loss 1.1514 | test ppl     3.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 14.20s | test loss 1.1420 | test ppl     3.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 14.22s | test loss 1.1114 | test ppl     3.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 14.19s | test loss 1.1021 | test ppl     3.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 14.18s | test loss 1.0901 | test ppl     2.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 14.16s | test loss 1.0798 | test ppl     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 14.18s | test loss 1.0690 | test ppl     2.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 14.19s | test loss 1.0614 | test ppl     2.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 14.17s | test loss 1.0470 | test ppl     2.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 14.17s | test loss 1.0348 | test ppl     2.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 14.18s | test loss 1.0320 | test ppl     2.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 14.19s | test loss 1.0315 | test ppl     2.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 14.20s | test loss 1.0192 | test ppl     2.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 14.17s | test loss 1.0101 | test ppl     2.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 14.19s | test loss 1.0140 | test ppl     2.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 14.18s | test loss 0.9989 | test ppl     2.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 14.19s | test loss 1.0056 | test ppl     2.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 14.17s | test loss 0.9971 | test ppl     2.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 14.20s | test loss 0.9870 | test ppl     2.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 14.20s | test loss 0.9875 | test ppl     2.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 14.20s | test loss 0.9834 | test ppl     2.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 14.17s | test loss 0.9793 | test ppl     2.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 14.18s | test loss 0.9787 | test ppl     2.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 14.19s | test loss 0.9841 | test ppl     2.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 14.21s | test loss 0.9622 | test ppl     2.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 14.18s | test loss 0.9653 | test ppl     2.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 14.19s | test loss 0.9738 | test ppl     2.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 14.18s | test loss 0.9673 | test ppl     2.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 14.20s | test loss 0.9606 | test ppl     2.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 14.17s | test loss 0.9732 | test ppl     2.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 14.17s | test loss 0.9490 | test ppl     2.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 14.19s | test loss 0.9546 | test ppl     2.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 14.19s | test loss 0.9562 | test ppl     2.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 14.17s | test loss 0.9510 | test ppl     2.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 14.17s | test loss 0.9552 | test ppl     2.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 14.18s | test loss 0.9491 | test ppl     2.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 14.17s | test loss 0.9415 | test ppl     2.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 14.18s | test loss 0.9541 | test ppl     2.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 14.18s | test loss 0.9486 | test ppl     2.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 14.17s | test loss 0.9472 | test ppl     2.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 14.17s | test loss 0.9512 | test ppl     2.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 14.19s | test loss 0.9470 | test ppl     2.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 14.18s | test loss 0.9450 | test ppl     2.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 14.17s | test loss 0.9380 | test ppl     2.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 14.17s | test loss 0.9336 | test ppl     2.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 14.18s | test loss 0.9322 | test ppl     2.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 14.17s | test loss 0.9336 | test ppl     2.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 14.17s | test loss 0.9288 | test ppl     2.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time: 14.19s | test loss 0.9273 | test ppl     2.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time: 14.17s | test loss 0.9298 | test ppl     2.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time: 14.18s | test loss 0.9161 | test ppl     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 104 | time: 14.17s | test loss 0.9206 | test ppl     2.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 105 | time: 14.19s | test loss 0.9233 | test ppl     2.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 106 | time: 14.16s | test loss 0.9159 | test ppl     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 107 | time: 14.19s | test loss 0.9177 | test ppl     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 108 | time: 14.18s | test loss 0.9098 | test ppl     2.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 109 | time: 14.19s | test loss 0.9150 | test ppl     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 110 | time: 14.16s | test loss 0.9098 | test ppl     2.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 111 | time: 14.17s | test loss 0.9078 | test ppl     2.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time: 14.18s | test loss 0.9047 | test ppl     2.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time: 14.18s | test loss 0.9047 | test ppl     2.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time: 14.17s | test loss 0.9096 | test ppl     2.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time: 14.18s | test loss 0.9003 | test ppl     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time: 14.17s | test loss 0.9030 | test ppl     2.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time: 14.18s | test loss 0.9021 | test ppl     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time: 14.19s | test loss 0.8988 | test ppl     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time: 14.18s | test loss 0.9026 | test ppl     2.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time: 14.16s | test loss 0.8989 | test ppl     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time: 14.17s | test loss 0.8982 | test ppl     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time: 14.17s | test loss 0.8927 | test ppl     2.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time: 14.20s | test loss 0.8917 | test ppl     2.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time: 14.20s | test loss 0.8891 | test ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time: 14.19s | test loss 0.8866 | test ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time: 14.18s | test loss 0.8866 | test ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time: 14.18s | test loss 0.8857 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time: 14.20s | test loss 0.8828 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time: 14.19s | test loss 0.8849 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time: 14.18s | test loss 0.8821 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time: 14.19s | test loss 0.8803 | test ppl     2.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time: 14.16s | test loss 0.8801 | test ppl     2.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time: 14.16s | test loss 0.8921 | test ppl     2.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time: 14.17s | test loss 0.8895 | test ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time: 14.17s | test loss 0.8831 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time: 14.18s | test loss 0.8846 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time: 14.18s | test loss 0.8859 | test ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time: 14.19s | test loss 0.8827 | test ppl     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time: 14.20s | test loss 0.8675 | test ppl     2.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time: 14.18s | test loss 0.8736 | test ppl     2.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time: 14.18s | test loss 0.8628 | test ppl     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time: 14.17s | test loss 0.8716 | test ppl     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time: 14.18s | test loss 0.8683 | test ppl     2.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time: 14.17s | test loss 0.8649 | test ppl     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time: 14.20s | test loss 0.8618 | test ppl     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time: 14.18s | test loss 0.8629 | test ppl     2.37\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-22-414210c630fa>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m     \u001B[0mepoch_start_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[1;31m# TODO: put all images into tensorboard so we can watch the attention shift\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-21-9b785c7c0b38>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mcurrent_loss\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalculate_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0mcurrent_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclip_grad_norm_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis_env\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m    130\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 132\u001B[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    133\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_over_time = []\n",
    "\n",
    "# TRAINING TIME----------------------------------------------------------------\n",
    "max_epochs = 1000 # If convergence isn't reached when reached we just stop\n",
    "warmup_epochs = 400\n",
    "convergence_grace_epochs = 80000\n",
    "\n",
    "# BATCHING & DECODER_SEQ_LENGHT------------------------------------------------\n",
    "#batch_size = 128\n",
    "batch_size = 256\n",
    "#batch_size = 512\n",
    "#batch_size = 640\n",
    "#batch_size = 896\n",
    "#batch_size = 1024\n",
    "#batch_size = 1536\n",
    "#batch_size = 2048\n",
    "decoder_seq_length = 16\n",
    "\n",
    "# MODEL HYPERPARAMS------------------------------------------------------------\n",
    "embedding_dimension = 768 # the size of vocabulary\n",
    "model_dimension = 256\n",
    "feedforward_dimension = 2048\n",
    "encoder_layers = 2 # 2 performed best\n",
    "decoder_layers = 2 # subject to testing\n",
    "heads = 4 # the number of heads for multiheadattention, 4 performed best\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "# MODEL INSTANTIATION----------------------------------------------------------\n",
    "model = BertHistoryTransformer(\n",
    "    decoder_seq_length=decoder_seq_length,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    model_dimension=model_dimension,\n",
    "    encoder_layers=encoder_layers,\n",
    "    decoder_layers=decoder_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# LR & TRAINING LOOP & OPTIMIZER-----------------------------------------------\n",
    "lr = 0.02\n",
    "\n",
    "def calculate_lr(step: int) -> float:\n",
    "    step += 1\n",
    "    return lr * min(pow(step, -0.5), step * pow(warmup_epochs, -1.5))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, calculate_lr)\n",
    "\n",
    "best_test_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "no_improvement_epoch_count = 0\n",
    "\n",
    "print('| ==== Got {:6d} training examples, splitting into {:4d} batches a {:4d} examples '.format(\n",
    "    train_data.shape[1], train_data.shape[1] // batch_size, batch_size\n",
    "))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "seq_length = 16 # this is to be adjusted in the scala code\n",
    "max_seq_length = 18\n",
    "\n",
    "similar_dissimilar_label = torch.ones(decoder_seq_length * batch_size * embedding_dimension).to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(1, max_epochs):\n",
    "    # check if we've converged\n",
    "    if no_improvement_epoch_count > convergence_grace_epochs:\n",
    "        print(\"| ----------------------- CONVERGENCE GRACE PERIOD SURPASSED\")\n",
    "        break\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "\n",
    "    # TODO: put all images into tensorboard so we can watch the attention shift\n",
    "    test_loss, enc_att, dec_att, mult_att = evaluate(model, test_data)\n",
    "    loss_over_time.append(test_loss)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.4f} | test ppl {:8.2f}'.format(\n",
    "        epoch, (time.time() - epoch_start_time), test_loss, math.exp(test_loss))\n",
    "    )\n",
    "    print('-' * 89)\n",
    "\n",
    "\n",
    "    if math.isclose(test_loss, best_test_loss, abs_tol=0.0001):\n",
    "        no_improvement_epoch_count += 1\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iUxA4c8i5Z7a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | out of sample loss 0.875252 | validation ppl     2.40\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "val_loss, a, b, c = evaluate(best_model, val_data)\n",
    "\n",
    "print('=' * 89)\n",
    "print('| End of training | out of sample loss {:5.6f} | validation ppl {:8.2f}'.format(val_loss, math.exp(val_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x28c2bf05a88>]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#torch.set_printoptions(edgeitems=3)\n",
    "\n",
    "batch_size = 3\n",
    "x, z, actual, pad = get_batch(val_data, test_padding_map, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[-9.0457e-01,  1.2714e-02,  4.9749e-01,  ...,  1.8895e+00,\n           8.6814e-01, -3.7130e-01],\n         [-1.7216e-01, -4.6049e-01,  2.5054e-01,  ...,  1.2910e+00,\n           1.2516e+00,  5.0627e-01],\n         [-4.8508e-01, -5.1946e-02,  1.8242e-01,  ...,  7.7027e-01,\n           7.8208e-01, -8.9879e-03]],\n\n        [[-5.3514e-01, -2.8362e-01,  9.0634e-01,  ...,  8.5750e-01,\n           8.7612e-01, -1.7978e-01],\n         [-2.5939e-03,  1.2943e-01, -1.0862e-01,  ...,  6.9449e-02,\n           6.6726e-01, -7.2774e-02],\n         [-3.0401e-01, -6.2218e-02,  6.5759e-01,  ...,  4.7975e-01,\n           9.8095e-01,  5.0123e-01]],\n\n        [[-7.3030e-01, -1.7939e-01,  2.3337e-01,  ...,  7.3753e-01,\n           9.7322e-01,  8.1784e-02],\n         [-5.4677e-01,  4.0596e-01,  2.7073e-01,  ...,  4.4434e-01,\n           6.5584e-01,  8.3244e-02],\n         [-2.4833e-01, -6.9638e-01,  4.4997e-01,  ...,  7.6136e-01,\n           2.2312e-01,  1.9022e-01]],\n\n        ...,\n\n        [[-5.7896e-01,  1.7959e-01, -6.0843e-02,  ...,  4.8247e-01,\n           5.8036e-01,  2.2410e-02],\n         [-6.4225e-02,  2.3623e-01,  3.2071e-01,  ...,  1.0403e+00,\n           5.0808e-01,  3.3165e-01],\n         [-4.7864e-01, -2.7348e-01, -5.9755e-01,  ...,  8.8892e-01,\n           9.4927e-01, -2.5962e-01]],\n\n        [[-5.4277e-01, -1.8050e+00,  3.3374e-01,  ...,  1.0744e+00,\n           7.1525e-01, -4.0009e-01],\n         [-5.6901e-01,  6.8492e-02,  1.6255e-01,  ...,  1.2121e+00,\n           5.7296e-01,  6.3125e-01],\n         [-9.1403e-01, -4.6243e-01, -4.5143e-01,  ...,  1.6715e+00,\n           1.3085e+00,  3.0819e-01]],\n\n        [[-3.4650e-01,  2.1501e-01,  1.0051e-01,  ...,  4.2101e-01,\n           4.8815e-02, -2.5998e-02],\n         [ 5.8235e-04, -5.3996e-01,  8.4309e-01,  ...,  3.3048e-01,\n           2.2942e-01,  2.0584e-01],\n         [-8.2027e-02,  2.8646e-01, -7.0131e-01,  ...,  7.3236e-01,\n           7.8370e-01,  1.9763e-01]]], device='cuda:0')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(actual.shape)\n",
    "actual"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Attention\n",
      "Decoder Attention\n",
      "Encoder-Decoder Attention\n",
      "0.9233057498931885\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction, enc_att, dec_att, mult_attn = best_model.predict(x, z, pad)\n",
    "\n",
    "plt.imshow(enc_att[0, 0, :, :].squeeze().detach().cpu().numpy(), cmap='Blues')\n",
    "print(\"Encoder Attention\")\n",
    "plt.show()\n",
    "plt.imshow(dec_att[0, :, :].squeeze().detach().cpu().numpy(), cmap='Blues')\n",
    "print(\"Decoder Attention\")\n",
    "plt.show()\n",
    "plt.imshow(mult_attn[0, :, :].squeeze().detach().cpu().numpy(), cmap='Blues')\n",
    "print(\"Encoder-Decoder Attention\")\n",
    "plt.show()\n",
    "\n",
    "loss = calculate_loss(prediction, actual)\n",
    "print(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([-1.0717, -0.8808, -0.9046, -0.5351, -0.7303, -0.0688,  0.2847,  0.3518,\n",
      "         0.0243, -0.3431, -0.1168, -0.7088, -0.1699, -0.6593, -0.1676, -0.5790],\n",
      "       device='cuda:0')\n",
      "z:\n",
      "tensor([-0.8808, -0.9046, -0.5351, -0.7303, -0.0688,  0.2847,  0.3518,  0.0243,\n",
      "        -0.3431, -0.1168, -0.7088, -0.1699, -0.6593, -0.1676, -0.5790, -0.5428],\n",
      "       device='cuda:0')\n",
      "----------------------------\n",
      "actual:\n",
      "tensor([[-0.9046,  0.0127,  0.4975],\n",
      "        [-0.5351, -0.2836,  0.9063],\n",
      "        [-0.7303, -0.1794,  0.2334],\n",
      "        [-0.0688, -0.8595,  0.5319],\n",
      "        [ 0.2847, -0.5828,  0.3962],\n",
      "        [ 0.3518,  0.7067,  0.7314],\n",
      "        [ 0.0243, -0.2605, -0.1588],\n",
      "        [-0.3431,  0.0356,  0.2591],\n",
      "        [-0.1168,  0.5269, -0.3710],\n",
      "        [-0.7088, -1.0321, -0.0255],\n",
      "        [-0.1699, -0.2010,  0.1341],\n",
      "        [-0.6593, -0.5567, -0.0046],\n",
      "        [-0.1676,  0.1457,  0.0438],\n",
      "        [-0.5790,  0.1796, -0.0608],\n",
      "        [-0.5428, -1.8050,  0.3337],\n",
      "        [-0.3465,  0.2150,  0.1005]], device='cuda:0')\n",
      "pred:\n",
      "tensor([[-0.1478, -0.2275,  0.3533],\n",
      "        [-0.2274, -0.1721,  0.2835],\n",
      "        [-0.3571, -0.1208,  0.2617],\n",
      "        [-0.4160, -0.1950,  0.2031],\n",
      "        [-0.3149, -0.0815,  0.2716],\n",
      "        [-0.3945, -0.2174,  0.2257],\n",
      "        [-0.2579, -0.1560,  0.2923],\n",
      "        [-0.2541, -0.0859,  0.2033],\n",
      "        [-0.3490, -0.1070,  0.1942],\n",
      "        [-0.3449, -0.1143,  0.1933],\n",
      "        [-0.2597, -0.2737,  0.1831],\n",
      "        [-0.1824, -0.1414,  0.1891],\n",
      "        [-0.3267, -0.1004,  0.2205],\n",
      "        [-0.3493, -0.1218,  0.1969],\n",
      "        [-0.2715, -0.1525,  0.2172],\n",
      "        [-0.3271, -0.2123,  0.1226]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"x:\")\n",
    "print(x[:, 0, 0])\n",
    "print(\"z:\")\n",
    "print(z[:, 0, 0])\n",
    "print(\"----------------------------\")\n",
    "print(\"actual:\")\n",
    "print(actual[:, 0, 0:3])\n",
    "print(\"pred:\")\n",
    "print(prediction[:, 0, 0:3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a lookup table for calculating recall and accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def extract_article_bert_tuple(data):\n",
    "    return { article_and_bert[\"_1\"]: article_and_bert[\"_2\"] for article_and_bert in data }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_lookup = read_multi_json_objects_file(\"../data/lookup.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "article_bert_lookup = extract_article_bert_tuple(raw_lookup)\n",
    "\n",
    "bert_vectors: List[List[float]] = list(article_bert_lookup.values())\n",
    "article_ids: List[int]          = list(article_bert_lookup.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "pred = []\n",
    "real = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(decoder_seq_length):\n",
    "        pred.append(prediction[j, i, :].detach().cpu().numpy())\n",
    "        real.append(actual[j, i, :].detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def get_calculated_similarities(bert_pred) -> List[List[float]]:\n",
    "    return cosine_similarity(bert_pred, bert_vectors).tolist()\n",
    "\n",
    "def get_article_for(similarities: List[float]) -> int:\n",
    "    article_id_index = similarities.index(max(similarities))\n",
    "    return article_ids[article_id_index]\n",
    "\n",
    "def get_actual_article(bert_actual: List[float]) -> int:\n",
    "    article_id_index = bert_vectors.index(bert_actual)\n",
    "    return article_ids[article_id_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "pred_similarities: List[List[float]] = get_calculated_similarities(pred)\n",
    "articles_pred: List[int] = [ get_article_for(pred_similarity) for pred_similarity in pred_similarities ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "true_similarities: List[List[float]] = get_calculated_similarities(real)\n",
    "articles_true: List[int] = [ get_article_for(true_similarity) for true_similarity in true_similarities ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710644733, 710644733, 710644733, 710644733, 734062063, 731868563, 734062063, 734062063, 710644733, 227552441, 227552441, 710644733, 710644733, 710644733, 710644733, 737480976, 710644733, 666216061, 666216061, 710644733, 710644733, 710644733, 737480976, 710644733, 737480976, 710644733, 710644733, 666216061, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733, 710644733]\n",
      "[731227949, 731452667, 738928041, 561377567, 738883014, 688037077, 740030469, 742062385, 705882024, 736150288, 741840092, 727231175, 703055624, 683718636, 742524127, 742558748, 700752395, 740884807, 715665631, 741170897, 741408143, 742044694, 742398083, 741645955, 740114922, 708343797, 732090776, 725478662, 741673105, 742068951, 669289018, 726374141, 741116757, 733960089, 729569284, 725923921, 724456012, 740884807, 738798794, 741170897, 607660747, 700759122, 742398083, 742651845, 610228515, 732583526, 727603433, 721167333]\n"
     ]
    }
   ],
   "source": [
    "print(articles_pred)\n",
    "print(articles_true)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(articles_true, articles_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "next_article_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}